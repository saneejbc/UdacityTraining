# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
 The objective of this project is to train a ML classifier model that can predict the category (binary class) of an individual's socieconomic information (age, job, marital status etc) in a bank marketing survey data. The training data given for the project was having a labelled data with a sample size of about 33000, each having 20 raw features of the individual's socio-economic survey records and class labels being either of the two yes/no

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**

Two Azure ML training methods were attempted on the training data 
 - Logistic regression using SKLearn package and hyeprdrive tuning of hyperparameters
 - Automated ML using Azure's standard autML pipeline

The best model generated from either of the two methods had a training performance of about 91% prediction acciracy, with very marginal difference between Hyperdrive vs AutoML. 

The **Logistic regression model** with the best accuracy (91.09%) was converged with the following parameters by hyperdrive: 

*['--C', '0.7324467412497473', '--max_iter', '250']*

The best perfroming **AutoML model** was a *VotingEnsemble* XGBoost Classifier with an accuracy of 91.73%, marginally higher than the best accuracy of Logistic Regression model from hyperdrive

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

**What are the benefits of the parameter sampler you chose?**
*'--C': uniform(0,1),
'--max_iter': choice(200, 250, 400, 500)*

Parameters: (C is for regulaization and max_iter is the maxium # of iterations)

C -  Unformly sampled from [0,1] so that the hyperdrive can optimize the regularization of logistic regression weights to balnce ebwetween over-fitting / under-fitting. the uniform sampler will reduce/minimize biasing the parameter search towards stronger regularization (underfitting) nor weaker regularization (overfitting)

max_iter:  Discrete choices to consider scenarios of fast and slow convergence of the model fit measure objective function.

**What are the benefits of the early stopping policy you chose?**

*policy = BanditPolicy(evaluation_interval=10, slack_factor=0.2)*

The above policy will check the model metric to be within a factor of 0.2 from the best observed run, for every 10 iterations in the current run - if the best metric for the current is below the best performing past run by a more than 20%, then the current run will be terminated prematurely (before the max number of iterations) - this saves the computational time spent on poor optimization iterations that can occur with such algorithms.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

The best perfroming model was an Ensemble method classifier with XGBoost learning technique.

The hyperparameters that were fixed for training wer: 

 *primary_metric= 'AUC_weighted', n_cross_validations=5*


The  model parameters was extracted from the generated model run as listed below:

    "class_name": "XGBoostClassifier",
    "module": "automl.client.core.common.model_wrappers",
    "param_args": [],
    "param_kwargs": {
        "booster": "gbtree",
        "colsample_bytree": 1,
        "eta": 0.05,
        "gamma": 0,
        "max_depth": 6,
        "max_leaves": 0,
        "n_estimators": 200,
        "objective": "reg:logistic",
        "reg_alpha": 0.625,
        "reg_lambda": 0.8333333333333334,
        "subsample": 0.8,
        "tree_method": "auto"
    },
    "prepared_kwargs": {},
    "spec_class": "sklearn"
}


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The performance differences between the best model genrated by Hyperdrive parameter search and the Auto ML was only marginal. However, configuring the pipeline for AutoML was significantly low effort compared to the coding steps required for the Hyperdrive. For example, the train.py script is not required for the AutoML method. On the other hand, the hyperdrive method gives more transparent handles for an experienced SME / domain expert on the use-case (someone who has significant apriori knowledge about the data generating mechanism and the use-case definition) - the customized training script and the model structure choice (for example, logistic regression) makes the model output/results from a hyperdrive emthodology more explainable than relying on a bruteforce model search method by AutoML.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
One important area of attention is feature selection / data-transformation methods - the trial exercises in this project relied on automatic feature slection available with the standard packages. Pre-processing/transforming  the data is also an important step that can have a significant impact on the accuracy of classification algorithms.

The Hyperdrive method can be tested with other classifer algorithms such as SVM, Decision tree etc. Also, the AutoML classifier can be trialled with more choices of cross-validation and primary_metrics (for example, we cna choose from 'average_precision_score_weighted','AUC_weighted', 'precision_score_weighted',  'accuracy', 'norm_macro_recall')

